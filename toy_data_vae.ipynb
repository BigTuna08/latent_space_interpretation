{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "- Toy data, in this version seqs are treated as seqs of floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chages, l, vocab, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toy_data-float-predlen-even-s-good-Copy1  \n",
    "uses no teacher force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "save_loc = \"saved/toy_seqs\"\n",
    "\n",
    "reps = 1\n",
    "ep = 1 #60\n",
    "EARLY_STOP = 3\n",
    "bs= 32\n",
    "\n",
    "## Full\n",
    "layer_size_options = [[256, 16], [512, 64, 16], [1024, 64]]\n",
    "latent_dim_options = [4, 6, 8]\n",
    "learning_rate_options = [5e-5]\n",
    "optimizer_options = [tf.keras.optimizers.Adam]\n",
    "\n",
    "# Quick \n",
    "layer_size_options = [[256, 16]]\n",
    "latent_dim_options = [4]\n",
    "learning_rate_options = [5e-5]\n",
    "optimizer_options = [tf.keras.optimizers.Adam]\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from my_lib.save_load import load_datasets, save_seq_vae\n",
    "from my_lib.models.vae_sequence import VAE, SCALE, START, loss_function, acc_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to load .ipynb_checkpoints (This is probably fine, unless it's needed)\n",
      "\n",
      "Columns of s are: start, stop, step, length\n",
      "\n",
      "initialized variable, s_cv\n",
      "initialized variable, s_tr\n",
      "\n",
      "Max seq lenght is 29.0\n"
     ]
    }
   ],
   "source": [
    "all_data = load_datasets(save_loc)\n",
    "\n",
    "print(\"\\nColumns of s are: start, stop, step, length\\n\")\n",
    "for name, model in all_data.items():\n",
    "    name = name.split(\".\")[0]\n",
    "    exec(\"{} = model\".format(name))\n",
    "    print(\"initialized variable,\", name)\n",
    "    \n",
    "max_len = max(s_tr[:,-1])\n",
    "print(\"\\nMax seq lenght is\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seqs = []\n",
    "for s in (s_tr, s_cv):\n",
    "    eff_stops = s[:,0] + s[:, 2]*max_len\n",
    "    expanded_seqs = [(x[0],eff_s, x[2] ) for x, eff_s in zip(s, eff_stops)]\n",
    "    all_seqs.append(expanded_seqs)\n",
    "    \n",
    "    \n",
    "all_seqs_tr, all_seqs_cv = all_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95000, 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train = tf.keras.preprocessing.sequence.pad_sequences([np.arange(*x) for x in all_seqs_tr], \n",
    "                                                          padding='post')\n",
    "input_tensor_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 29)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_val = tf.keras.preprocessing.sequence.pad_sequences([np.arange(*x) for x in all_seqs_cv], \n",
    "                                                          padding='post')\n",
    "input_tensor_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train = tf.cast(input_tensor_train.reshape(*input_tensor_train.shape, 1), tf.float32)\n",
    "input_tensor_val = tf.cast(input_tensor_val.reshape(*input_tensor_val.shape, 1), tf.float32)\n",
    "\n",
    "\n",
    "seq_lens_train = tf.constant(s_tr[:,-1], dtype=tf.float32)\n",
    "seq_lens_val = tf.constant(s_cv[:,-1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train/SCALE, seq_lens_train)) \\\n",
    "                         .shuffle(len(input_tensor_train)) \\\n",
    "                         .batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset_cv = tf.data.Dataset.from_tensor_slices((input_tensor_val/SCALE, seq_lens_val)) \\\n",
    "                            .shuffle(len(input_tensor_val)) \\\n",
    "                            .batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconsturction(ae, max_show=10, teacher_forcing=True, cv_data=True):\n",
    "\n",
    "    encoder= ae.encoder\n",
    "    decoder = ae.decoder\n",
    "    BATCH_SIZE = 32\n",
    "    sentances = [[] for _ in range(BATCH_SIZE)]\n",
    "\n",
    "    if cv_data:\n",
    "        for (inp, true_lens) in dataset_cv: break # hacky way to get inp, targ\n",
    "    else:\n",
    "        for (inp, true_lens) in dataset: break # hacky way to get inp, targ\n",
    "    to_translate = np.zeros(inp.shape)\n",
    "\n",
    "    loss = 0\n",
    "    acc = tf.constant([0,0], dtype=tf.float32)\n",
    "\n",
    "    masked_inp = inp*tf.expand_dims(tf.sequence_mask(true_lens, dtype=tf.float32, maxlen=inp.shape[1]),-1)\n",
    "    context, _, _  = encoder(masked_inp)\n",
    "    \n",
    "    dec_input = tf.expand_dims([START] * ae.batch_size, 1)\n",
    "    dec_hidden = ae.initialize_hidden_state()\n",
    "\n",
    "    pred_list = []\n",
    "    pred_lens = tf.minimum(tf.round(ae.len_decoder(context)),inp.shape[1])\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(inp.shape[1]):\n",
    "        # passing enc_output to the self.decoder\n",
    "        predictions, dec_hidden, = decoder((dec_input, context, dec_hidden))\n",
    "\n",
    "        loss += loss_function(inp[:, t], predictions , pred_lens)\n",
    "        acc += acc_metric(inp[:, t], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        \n",
    "        to_translate[:,t] = predictions\n",
    "        \n",
    "        if teacher_forcing:\n",
    "            dec_input = inp[:, t]\n",
    "        else:\n",
    "            dec_input = predictions\n",
    "            \n",
    "        pred_list.append(predictions)\n",
    "\n",
    "    batch_loss = (loss / int(inp.shape[1]))\n",
    "\n",
    "    \n",
    "    print(\"*\"*100)\n",
    "    print(\"acc\", 1- acc[0]/acc[1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    for i in range(min(max_show, len(inp))):\n",
    "\n",
    "        true = [x[0] for x in tf.round(inp[i]*SCALE).numpy() ]#if x[0] > 0]\n",
    "        pred = [x[0] for x in tf.round(to_translate[i]*SCALE).numpy()] #[:int(pred_lens[i])]\n",
    "\n",
    "        true_fin = False\n",
    "        pred_fin = False\n",
    "\n",
    "        for seq_i, (t,p) in enumerate(zip(true, pred)):\n",
    "            if seq_i < true_lens[i]: \n",
    "                t = str(int(t))\n",
    "            else:\n",
    "                t = \"   \"\n",
    "                true_fin = True\n",
    "            if seq_i < pred_lens[i]:\n",
    "                p = str(int(p))\n",
    "            else:\n",
    "                p = \"   \"\n",
    "                pred_fin = True\n",
    "\n",
    "            if true_fin and pred_fin:\n",
    "                break\n",
    "\n",
    "            print(\"{} ----> {}\".format(t, p))\n",
    "\n",
    "        true_s = \"\\t\".join(map(str,map(int, true)))\n",
    "        pred_s = \"\\t\".join(map(str,map(int, pred)))\n",
    "\n",
    "\n",
    "        print()\n",
    "        print(\"true step:\", true[1]-true[0])\n",
    "        print(\"mean pred step {:.2f}\".format(np.mean(np.diff(pred[:int(pred_lens[i])]))))\n",
    "        print(\"median pred step {:.2f}\".format(np.median(np.diff(pred[:int(pred_lens[i])]))))\n",
    "\n",
    "        tl = true_lens[i].numpy()\n",
    "        pl = pred_lens[i].numpy()[0]\n",
    "        print(\"True length:\",  tl)\n",
    "        print(\"Predicted length:\", pl)\n",
    "\n",
    "        print(\"\\n\", \"*\"*100 ,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_ae(ae, EPOCHS, EARLY_STOP=4, ppe = 2, no_tf_p=0.02, big_update_every=4):\n",
    "    \n",
    "    steps_per_epoch = len(dataset) \n",
    "    steps_per_epoch_cv = len(dataset_cv) \n",
    "    \n",
    "    print_every = steps_per_epoch // ppe\n",
    "    full_start = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        update_count = 0 \n",
    "        start = time.time()\n",
    "\n",
    "        total_loss, cv_total_loss = 0, 0\n",
    "        total_acc, cv_total_acc = 0,0\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(dataset):\n",
    "            \n",
    "            # randomly choose if using teacher forcing\n",
    "            if np.random.random() > no_tf_p:\n",
    "                batch_loss, batch_mets = ae.train_step(inp, targ)\n",
    "            else:\n",
    "                batch_loss, batch_mets = ae.train_step_tf(inp, targ)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "\n",
    "            if batch % print_every == 0:\n",
    "                update_count += 1\n",
    "                print('Epoch {} update {} (Batch {}) Loss {:.4f}'.format(epoch + 1,\n",
    "                                                                         update_count,\n",
    "                                                             batch,\n",
    "                                                             batch_loss.numpy()))\n",
    "                \n",
    "                for name, val in batch_mets:\n",
    "                    print('Epoch {} update {} (Metric={}) val={:.4f}'.format(epoch + 1,\n",
    "                                                                            update_count,\n",
    "                                                     name,\n",
    "                                                     val))\n",
    "                \n",
    "            \n",
    "\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch) )\n",
    "        print('Epoch {} Acc {:.4f}'.format(epoch + 1, total_acc / steps_per_epoch))\n",
    "\n",
    "        ae.track[\"loss_tr\"].append(total_loss / steps_per_epoch)\n",
    "        ae.track[\"acc_tr\"].append(total_acc / steps_per_epoch)\n",
    "\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(dataset_cv.take(steps_per_epoch_cv)):\n",
    "\n",
    "            batch_loss, batch_mets = ae.cv_step(inp, targ)\n",
    "            cv_total_loss += batch_loss  \n",
    "\n",
    "            if batch == 0:\n",
    "                cv_info = {}\n",
    "                for name, val in batch_mets:\n",
    "                    name = name.numpy().decode(\"utf-8\")\n",
    "                    cv_info[name] = val                    \n",
    "            else:\n",
    "                for name, val in batch_mets:\n",
    "                    name = name.numpy().decode(\"utf-8\")\n",
    "                    cv_info[name] += val\n",
    "                \n",
    "\n",
    "        print('**CV** Epoch {} Loss {:.4f}'.format(epoch + 1, cv_total_loss / steps_per_epoch_cv))\n",
    "\n",
    "        ae.track[\"loss_cv\"].append(cv_total_loss / steps_per_epoch_cv)\n",
    "        \n",
    "        for name, val in cv_info.items():\n",
    "\n",
    "\n",
    "            print('Epoch {} update {} (Metric={}) val={:.4f}'.format(epoch + 1,\n",
    "                                                                    update_count,\n",
    "                                                                     name,\n",
    "                                                                     val / steps_per_epoch_cv))\n",
    "\n",
    "\n",
    "        print('Time taken for 1 epoch {} sec\\n\\n'.format(time.time() - start))\n",
    "        \n",
    "\n",
    "\n",
    "        if len(ae.track[\"loss_cv\"]) > EARLY_STOP and min(ae.track[\"loss_cv\"]) < min(ae.track[\"loss_cv\"][-EARLY_STOP:]):\n",
    "            print(\"\\nEarly stop, best loss\", min(ae.track[\"loss_cv\"]), \"last\", EARLY_STOP, ae.track[\"loss_cv\"][-EARLY_STOP:])\n",
    "            break \n",
    "\n",
    "            \n",
    "        if big_update_every:\n",
    "            if epoch % big_update_every == 0:\n",
    "\n",
    "                show_reconsturction(ae, max_show=2)\n",
    "                print(\"without teacher forcing\")\n",
    "                show_reconsturction(ae, max_show=2, teacher_forcing=False)\n",
    "\n",
    "\n",
    "\n",
    "    print('\\n\\nFull Time taken for {} epochs is {} sec\\n\\n'.format(epoch, time.time() - full_start))\n",
    "\n",
    "    return ae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********\tBegin trial\t0\t********\t\t\n",
      "Time taken for trace (train):\t\t29.36\n",
      "\n",
      "Time taken for trace (train):\t\t25.15\n",
      "\n",
      "Epoch 1 update 1 (Batch 0) Loss 201.9684\n",
      "Epoch 1 update 1 (Metric=b'acc_m') val=0.0000\n",
      "Epoch 1 update 1 (Metric=b'kl_m') val=0.0179\n",
      "Epoch 1 update 1 (Metric=b'len_m') val=3.6367\n",
      "Epoch 1 update 1 (Metric=b'recons_m') val=198.3138\n",
      "Time taken for trace (train):\t\t5.62\n",
      "\n",
      "Epoch 1 update 2 (Batch 1484) Loss 2.2757\n",
      "Epoch 1 update 2 (Metric=b'acc_m') val=0.0065\n",
      "Epoch 1 update 2 (Metric=b'kl_m') val=0.5924\n",
      "Epoch 1 update 2 (Metric=b'len_m') val=0.4640\n",
      "Epoch 1 update 2 (Metric=b'recons_m') val=1.2192\n",
      "Epoch 1 Loss 6.2551\n",
      "Epoch 1 Acc 0.0000\n",
      "Time taken for trace (train):\t\t1.92\n",
      "\n",
      "**CV** Epoch 1 Loss 1.5623\n",
      "Epoch 1 update 2 (Metric=acc_m) val=0.0108\n",
      "Epoch 1 update 2 (Metric=kl_m) val=0.4399\n",
      "Epoch 1 update 2 (Metric=len_m) val=0.3995\n",
      "Epoch 1 update 2 (Metric=recons_m) val=0.7228\n",
      "Time taken for 1 epoch 189.84444880485535 sec\n",
      "\n",
      "\n",
      "****************************************************************************************************\n",
      "acc tf.Tensor(0.007543087, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "224 ----> 112\n",
      "239 ----> 168\n",
      "254 ----> 223\n",
      "269 ----> 265\n",
      "    ----> 293\n",
      "    ----> 313\n",
      "    ----> 329\n",
      "    ----> 344\n",
      "\n",
      "true step: 15.0\n",
      "mean pred step 33.14\n",
      "median pred step 28.00\n",
      "True length: 4.0\n",
      "Predicted length: 8.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "944 ----> 412\n",
      "988 ----> 781\n",
      "1032 ----> 1022\n",
      "1076 ----> 1107\n",
      "1120 ----> 1135\n",
      "1164 ----> 1161\n",
      "1208 ----> 1196\n",
      "1252 ----> 1235\n",
      "1296 ----> 1277\n",
      "1340 ----> 1319\n",
      "1384 ----> 1362\n",
      "1428 ----> 1406\n",
      "1472 ----> 1450\n",
      "1516 ----> 1495\n",
      "1560 ----> 1539\n",
      "1604 ----> 1584\n",
      "1648 ----> 1629\n",
      "1692 ----> 1674\n",
      "1736 ----> 1719\n",
      "1780 ----> 1764\n",
      "1824 ----> 1809\n",
      "1868 ----> 1854\n",
      "1912 ---->    \n",
      "1956 ---->    \n",
      "\n",
      "true step: 44.0\n",
      "mean pred step 68.67\n",
      "median pred step 45.00\n",
      "True length: 24.0\n",
      "Predicted length: 22.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "without teacher forcing\n",
      "****************************************************************************************************\n",
      "acc tf.Tensor(0.0021551847, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "890 ----> 364\n",
      "912 ----> 497\n",
      "934 ----> 582\n",
      "956 ----> 641\n",
      "978 ----> 685\n",
      "1000 ----> 722\n",
      "1022 ----> 755\n",
      "1044 ----> 786\n",
      "1066 ----> 815\n",
      "1088 ----> 843\n",
      "1110 ----> 869\n",
      "1132 ----> 895\n",
      "1154 ----> 920\n",
      "1176 ----> 944\n",
      "1198 ----> 968\n",
      "1220 ----> 992\n",
      "1242 ----> 1016\n",
      "\n",
      "true step: 22.0\n",
      "mean pred step 40.75\n",
      "median pred step 28.50\n",
      "True length: 17.0\n",
      "Predicted length: 17.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "460 ----> 294\n",
      "463 ----> 378\n",
      "466 ----> 439\n",
      "469 ----> 484\n",
      "472 ----> 519\n",
      "475 ----> 548\n",
      "478 ----> 575\n",
      "481 ----> 600\n",
      "484 ----> 623\n",
      "487 ----> 645\n",
      "490 ----> 667\n",
      "493 ----> 688\n",
      "496 ----> 708\n",
      "499 ----> 727\n",
      "502 ----> 745\n",
      "505 ----> 763\n",
      "508 ----> 781\n",
      "511 ---->    \n",
      "514 ---->    \n",
      "\n",
      "true step: 3.0\n",
      "mean pred step 30.44\n",
      "median pred step 22.50\n",
      "True length: 19.0\n",
      "Predicted length: 17.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "\n",
      "\n",
      "Full Time taken for 0 epochs is 190.25674486160278 sec\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trial in range(reps):\n",
    "    for hidden_dim in layer_size_options:\n",
    "        for ld in latent_dim_options:\n",
    "            for op in optimizer_options:\n",
    "                for lr in learning_rate_options:\n",
    "                \n",
    "                    opt = op(learning_rate=lr)\n",
    "                    opt.name= opt.get_config()[\"name\"]+ str(lr).replace(\".\", \"_\")\n",
    "\n",
    "                    print(\"\\n\")\n",
    "                    print(\"*\"*8, \"Begin trial\", trial, \"*\"*8, \"\\t\", sep=\"\\t\")\n",
    "                    experiment_start = time.time()\n",
    "\n",
    "\n",
    "                    now = datetime.now()\n",
    "                    time_str = \"-\".join(map(str, [now.year, now.month, now.day, now.hour, now.minute]))\n",
    "\n",
    "                    ld_str = \"_\".join(map(str, hidden_dim)) + \"_{}\".format(ld)\n",
    "                    setting_str = \"bs-{}-ld-{}-ep-{}-opt-{}\".format(bs, ld_str, ep, opt.name)\n",
    "                    id_str = setting_str + \"_\" + time_str\n",
    "\n",
    "\n",
    "\n",
    "                    ae = VAE(inp_shape = input_tensor_train.shape[1:], \n",
    "                            latent_dim=ld,\n",
    "                            hidden_units=hidden_dim,\n",
    "                            batch_size=bs,\n",
    "                            optimizer=opt,\n",
    "                            start_word=START)\n",
    "\n",
    "\n",
    "                    aes.append((id_str, ae))\n",
    "                    train_ae(ae, ep, EARLY_STOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View trained networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =  1.5623237\n",
      "With Teacher Forcing\n",
      "****************************************************************************************************\n",
      "acc tf.Tensor(0.014008641, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "360 ----> 126\n",
      "372 ----> 230\n",
      "384 ----> 325\n",
      "396 ----> 389\n",
      "    ----> 425\n",
      "    ----> 445\n",
      "    ----> 457\n",
      "    ----> 467\n",
      "    ----> 477\n",
      "\n",
      "true step: 12.0\n",
      "mean pred step 43.88\n",
      "median pred step 28.00\n",
      "True length: 4.0\n",
      "Predicted length: 9.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "951 ----> 383\n",
      "987 ----> 749\n",
      "1023 ----> 993\n",
      "1059 ----> 1081\n",
      "1095 ----> 1108\n",
      "1131 ----> 1130\n",
      "1167 ----> 1160\n",
      "1203 ----> 1195\n",
      "1239 ----> 1231\n",
      "1275 ----> 1268\n",
      "1311 ----> 1304\n",
      "1347 ----> 1340\n",
      "1383 ----> 1377\n",
      "1419 ----> 1414\n",
      "1455 ----> 1450\n",
      "1491 ---->    \n",
      "\n",
      "true step: 36.0\n",
      "mean pred step 76.21\n",
      "median pred step 36.00\n",
      "True length: 16.0\n",
      "Predicted length: 15.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "131 ----> 212\n",
      "159 ----> 264\n",
      "187 ----> 284\n",
      "215 ----> 286\n",
      "243 ----> 287\n",
      "271 ----> 296\n",
      "299 ----> 312\n",
      "327 ----> 333\n",
      "355 ----> 357\n",
      "383 ----> 383\n",
      "411 ----> 408\n",
      "439 ---->    \n",
      "467 ---->    \n",
      "\n",
      "true step: 28.0\n",
      "mean pred step 19.60\n",
      "median pred step 20.50\n",
      "True length: 13.0\n",
      "Predicted length: 11.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "392 ----> 304\n",
      "410 ----> 416\n",
      "428 ----> 482\n",
      "446 ----> 506\n",
      "464 ----> 513\n",
      "482 ----> 519\n",
      "500 ----> 529\n",
      "518 ----> 541\n",
      "536 ----> 556\n",
      "554 ----> 572\n",
      "572 ----> 588\n",
      "590 ----> 604\n",
      "608 ----> 621\n",
      "626 ----> 638\n",
      "644 ----> 654\n",
      "662 ----> 671\n",
      "680 ----> 688\n",
      "698 ----> 704\n",
      "716 ---->    \n",
      "\n",
      "true step: 18.0\n",
      "mean pred step 23.53\n",
      "median pred step 16.00\n",
      "True length: 19.0\n",
      "Predicted length: 18.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Without Teacher Forcing\n",
      "****************************************************************************************************\n",
      "acc tf.Tensor(0.0010775924, shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "935 ----> 369\n",
      "982 ----> 512\n",
      "1029 ----> 601\n",
      "1076 ----> 661\n",
      "1123 ----> 706\n",
      "1170 ----> 744\n",
      "1217 ----> 778\n",
      "1264 ----> 810\n",
      "1311 ----> 839\n",
      "1358 ----> 868\n",
      "1405 ----> 896\n",
      "1452 ----> 923\n",
      "1499 ----> 950\n",
      "\n",
      "true step: 47.0\n",
      "mean pred step 48.42\n",
      "median pred step 33.00\n",
      "True length: 13.0\n",
      "Predicted length: 13.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "137 ----> 124\n",
      "176 ----> 151\n",
      "215 ----> 179\n",
      "254 ----> 206\n",
      "293 ----> 230\n",
      "332 ----> 252\n",
      "    ----> 273\n",
      "    ----> 294\n",
      "    ----> 315\n",
      "\n",
      "true step: 39.0\n",
      "mean pred step 23.88\n",
      "median pred step 23.00\n",
      "True length: 6.0\n",
      "Predicted length: 9.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "105 ----> 219\n",
      "140 ----> 278\n",
      "175 ----> 329\n",
      "210 ----> 369\n",
      "245 ----> 403\n",
      "280 ----> 431\n",
      "315 ----> 458\n",
      "350 ----> 482\n",
      "385 ----> 505\n",
      "420 ----> 528\n",
      "455 ----> 549\n",
      "490 ----> 570\n",
      "525 ---->    \n",
      "\n",
      "true step: 35.0\n",
      "mean pred step 31.91\n",
      "median pred step 27.00\n",
      "True length: 13.0\n",
      "Predicted length: 12.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "150 ----> 230\n",
      "154 ----> 284\n",
      "158 ----> 331\n",
      "162 ----> 369\n",
      "166 ----> 399\n",
      "170 ----> 426\n",
      "174 ----> 450\n",
      "178 ----> 472\n",
      "182 ----> 493\n",
      "186 ----> 514\n",
      "190 ----> 534\n",
      "194 ----> 553\n",
      "198 ----> 571\n",
      "202 ----> 589\n",
      "206 ----> 607\n",
      "210 ----> 623\n",
      "214 ---->    \n",
      "218 ---->    \n",
      "222 ---->    \n",
      "226 ---->    \n",
      "230 ---->    \n",
      "234 ---->    \n",
      "238 ---->    \n",
      "242 ---->    \n",
      "246 ---->    \n",
      "\n",
      "true step: 4.0\n",
      "mean pred step 26.20\n",
      "median pred step 21.00\n",
      "True length: 25.0\n",
      "Predicted length: 16.0\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sort_key(x):\n",
    "    try:\n",
    "        id_str, model = x\n",
    "        return min(model.track[\"loss_cv\"])\n",
    "    except:\n",
    "        return 1e10\n",
    "\n",
    "\n",
    "for id_str, ae in sorted(aes, key=sort_key):\n",
    "    if len(ae.track[\"loss_cv\"]) == 0: \n",
    "        continue\n",
    "\n",
    "\n",
    "    for key in ae.track:\n",
    "        if \"loss\" in key:\n",
    "            plt.plot(ae.track[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.title(id_str)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"min = \", min(ae.track[\"loss_cv\"]).numpy())\n",
    "\n",
    "    print(\"With Teacher Forcing\")\n",
    "    show_reconsturction(ae, max_show=4)\n",
    "\n",
    "    print(\"\\n\"*3)\n",
    "    print(\"Without Teacher Forcing\")\n",
    "    show_reconsturction(ae,  max_show=4, teacher_forcing=False)\n",
    "    print(\"\\n\"*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_models(save_loc, (ae.encoder, ae.decoder, ae.len_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: saved/toy_seqs/models/encoder/assets\n",
      "INFO:tensorflow:Assets written to: saved/toy_seqs/models/decoder/assets\n",
      "INFO:tensorflow:Assets written to: saved/toy_seqs/models/len_decoder/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_seq_vae(ae, save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
